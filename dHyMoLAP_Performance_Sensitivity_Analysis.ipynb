{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldQ4LEVQfUzQ"
      },
      "source": [
        "# HyMoLAP Performance Sensitivity Analysis to 5 catchments characteristics\n",
        "\n",
        "**Author:** Lionel Cedric Gohouede\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYdbA3aWfUzT",
        "outputId": "5f02525a-b1eb-4511-df6d-8b335c789a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Libraries loaded\n",
            "  XGBoost: 3.2.0\n",
            "  SHAP: 0.50.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# XGBoost and SHAP\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "\n",
        "# Machine learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, make_scorer, balanced_accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.size'] = 9\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "print(\"✓ Libraries loaded\")\n",
        "print(f\"  XGBoost: {xgb.__version__}\")\n",
        "print(f\"  SHAP: {shap.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbNcGDnTfUzU"
      },
      "source": [
        "## 1. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks/Data\")\n",
        "\n",
        "params = pd.read_csv(DATA_DIR / 'dHyMoLAP_Simulation_Data_CAMELS_FR.csv')\n",
        "climate = pd.read_csv(DATA_DIR / 'CAMELS_FR_climatic_statistics.csv', sep=';')\n",
        "hydro = pd.read_csv(DATA_DIR / 'CAMELS_FR_hydrological_signatures.csv', sep=';')\n",
        "hydro_yearly = pd.read_csv(DATA_DIR / 'CAMELS_FR_hydroclimatic_statistics_joint_availability_yearly.csv', sep=';')\n",
        "topo = pd.read_csv(DATA_DIR / 'CAMELS_FR_topography_general_attributes.csv', sep=';')\n",
        "geology = pd.read_csv(DATA_DIR / 'CAMELS_FR_geology_attributes.csv', sep=';')\n",
        "hydrogeology = pd.read_csv(DATA_DIR / 'CAMELS_FR_hydrogeology_attributes.csv', sep=';')\n",
        "landcover = pd.read_csv(DATA_DIR / 'CAMELS_FR_land_cover_attributes.csv', sep=';')\n",
        "station = pd.read_csv(DATA_DIR / 'CAMELS_FR_station_general_attributes.csv', sep=';', on_bad_lines='skip')\n",
        "site = pd.read_csv(DATA_DIR / 'CAMELS_FR_site_general_attributes.csv', sep=';', on_bad_lines='skip')\n",
        "nestedness = pd.read_csv(DATA_DIR / 'CAMELS_FR_catchment_nestedness_information.csv', sep=';')\n",
        "dams = pd.read_csv(DATA_DIR / 'CAMELS_FR_human_influences_dams.csv', sep=';')\n",
        "soil_raw = pd.read_csv(DATA_DIR / 'CAMELS_FR_soil_general_attributes.csv', sep=';')"
      ],
      "metadata": {
        "id": "ysLztUggtRyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip4shkVZfUzV",
        "outputId": "b34789bd-9ece-48bb-edae-823d7177ad90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: 549 catchments\n",
            "\n",
            "Class Distribution:\n",
            "  Unsatisfactory :  12.4% ( 68)\n",
            "  Satisfactory   :  44.8% (246)\n",
            "  Good           :  31.0% (170)\n",
            "  Very Good      :  11.8% ( 65)\n",
            "\n",
            "Imbalance ratio: 3.78:1\n"
          ]
        }
      ],
      "source": [
        "bins = [-np.inf, 0.50, 0.70, 0.80, 1.00]\n",
        "labels = ['Unsatisfactory', 'Satisfactory', 'Good', 'Very Good']\n",
        "params['Performance'] = pd.cut(params['NSE_val'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "print(f\"Dataset: {len(params)} catchments\")\n",
        "print(f\"\\nClass Distribution:\")\n",
        "for label in labels:\n",
        "    count = (params['Performance'] == label).sum()\n",
        "    pct = 100 * count / len(params)\n",
        "    print(f\"  {label:15s}: {pct:5.1f}% ({count:3d})\")\n",
        "\n",
        "imbalance_ratio = params['Performance'].value_counts().max() / params['Performance'].value_counts().min()\n",
        "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grH3KNzffUzW",
        "outputId": "31ab74ad-fd87-4b7b-eba1-608158dfe681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STANDARDIZING STATION IDs\n",
            "================================================================================\n",
            "  ✓ climate        : Renamed sta_code_h3 → station_id\n",
            "  ✓ hydro          : Renamed sta_code_h3 → station_id\n",
            "  ✓ hydro_yearly   : Renamed sta_code_h3 → station_id\n",
            "  ✓ topo           : Renamed sta_code_h3 → station_id\n",
            "  ✓ soil           : Renamed sta_code_h3 → station_id\n",
            "  ✓ geology        : Renamed sta_code_h3 → station_id\n",
            "  ✓ hydrogeology   : Renamed sta_code_h3 → station_id\n",
            "  ✓ landcover      : Renamed sta_code_h3 → station_id\n",
            "  ✓ nestedness     : Renamed sta_code_h3 → station_id\n",
            "  ✓ dams           : Renamed sta_code_h3 → station_id\n",
            "  ✓ station        : Renamed sta_code_h3 → station_id\n",
            "  ⚠️  site           : WARNING - No sit_code_h3 or station_id column!\n",
            "      Columns: ['sta_code_h3', 'sit_label', 'sit_mnemonic', 'sit_label_usual', 'sit_label_add']\n",
            "\n",
            "================================================================================\n",
            "MERGING DATASETS\n",
            "================================================================================\n",
            "\n",
            "Base data: 549 stations\n",
            "  + Climate        :  30 features added → Total:  33 columns\n",
            "  + Hydro          :  17 features added → Total:  50 columns\n",
            "  + Hydro_Yearly   :   8 features added → Total:  58 columns\n",
            "  + Topo           :  27 features added → Total:  85 columns\n",
            "  + Soil           :  33 features added → Total: 118 columns\n",
            "  + Geology        :  17 features added → Total: 135 columns\n",
            "  + Hydrogeology   :  11 features added → Total: 146 columns\n",
            "  + Landcover      :  16 features added → Total: 162 columns\n",
            "  + Station        :  48 features added → Total: 210 columns\n",
            "  ⚠️  SKIPPING Site           : No station_id column\n",
            "  + Nestedness     :   6 features added → Total: 216 columns\n",
            "  + Dams           :   3 features added → Total: 219 columns\n",
            "\n",
            "================================================================================\n",
            "FINAL MERGED DATASET\n",
            "================================================================================\n",
            "Catchments: 549\n",
            "Total columns: 219\n",
            "Features: 216 (excluding station_id, NSE_val, Performance)\n",
            "\n",
            "✅ Data merge complete\n"
          ]
        }
      ],
      "source": [
        "soil_mean = soil_raw[soil_raw['sol_stat'] == 'mean'].copy()\n",
        "\n",
        "soil_pivot_list = []\n",
        "for agg_level in ['no', 'topsoil', 'top_subsoil']:\n",
        "    subset = soil_mean[soil_mean['sol_agg_level'] == agg_level].copy()\n",
        "    subset = subset.drop(['sol_stat', 'sol_agg_level'], axis=1)\n",
        "    rename_dict = {col: f'sol_{agg_level}_{col}' if col != 'sta_code_h3' else col for col in subset.columns}\n",
        "    subset = subset.rename(columns=rename_dict)\n",
        "    soil_pivot_list.append(subset)\n",
        "\n",
        "soil = soil_pivot_list[0]\n",
        "for df in soil_pivot_list[1:]:\n",
        "    soil = soil.merge(df, on='sta_code_h3', how='outer')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STANDARDIZING STATION IDs\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Standardize IDs - Check each DataFrame\n",
        "datasets_to_rename = {\n",
        "    'climate': climate,\n",
        "    'hydro': hydro,\n",
        "    'hydro_yearly': hydro_yearly,\n",
        "    'topo': topo,\n",
        "    'soil': soil,\n",
        "    'geology': geology,\n",
        "    'hydrogeology': hydrogeology,\n",
        "    'landcover': landcover,\n",
        "    'nestedness': nestedness,\n",
        "    'dams': dams,\n",
        "    'station': station\n",
        "}\n",
        "\n",
        "for name, df in datasets_to_rename.items():\n",
        "    if 'sta_code_h3' in df.columns:\n",
        "        df.rename(columns={'sta_code_h3': 'station_id'}, inplace=True)\n",
        "        print(f\"  ✓ {name:15s}: Renamed sta_code_h3 → station_id\")\n",
        "    elif 'station_id' in df.columns:\n",
        "        print(f\"  ✓ {name:15s}: Already has station_id\")\n",
        "    else:\n",
        "        print(f\"  ⚠️  {name:15s}: WARNING - No sta_code_h3 or station_id column!\")\n",
        "        print(f\"      Columns: {list(df.columns)[:5]}\")\n",
        "\n",
        "# Site has different ID column\n",
        "if 'sit_code_h3' in site.columns:\n",
        "    site.rename(columns={'sit_code_h3': 'station_id'}, inplace=True)\n",
        "    print(f\"  ✓ {'site':15s}: Renamed sit_code_h3 → station_id\")\n",
        "elif 'station_id' in site.columns:\n",
        "    print(f\"  ✓ {'site':15s}: Already has station_id\")\n",
        "else:\n",
        "    print(f\"  ⚠️  {'site':15s}: WARNING - No sit_code_h3 or station_id column!\")\n",
        "    print(f\"      Columns: {list(site.columns)[:5]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MERGING DATASETS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Merge (EXCLUDE training metrics and parameters)\n",
        "data = params[['station_id', 'NSE_val', 'Performance']].copy()\n",
        "print(f\"\\nBase data: {len(data)} stations\")\n",
        "\n",
        "# Merge with verification\n",
        "for df, name in [(climate, 'Climate'), (hydro, 'Hydro'), (hydro_yearly, 'Hydro_Yearly'),\n",
        "                 (topo, 'Topo'), (soil, 'Soil'), (geology, 'Geology'),\n",
        "                 (hydrogeology, 'Hydrogeology'), (landcover, 'Landcover'),\n",
        "                 (station, 'Station'), (site, 'Site'), (nestedness, 'Nestedness'), (dams, 'Dams')]:\n",
        "\n",
        "    # Check if station_id exists before merging\n",
        "    if 'station_id' not in df.columns:\n",
        "        print(f\"  ⚠️  SKIPPING {name:15s}: No station_id column\")\n",
        "        continue\n",
        "\n",
        "    before_cols = data.shape[1]\n",
        "    data = data.merge(df, on='station_id', how='left', suffixes=('', '_dup'))\n",
        "\n",
        "    # Remove duplicate columns\n",
        "    data = data.loc[:, ~data.columns.str.endswith('_dup')]\n",
        "\n",
        "    after_cols = data.shape[1]\n",
        "    added_cols = after_cols - before_cols\n",
        "    print(f\"  + {name:15s}: {added_cols:3d} features added → Total: {after_cols:3d} columns\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"FINAL MERGED DATASET\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Catchments: {len(data)}\")\n",
        "print(f\"Total columns: {len(data.columns)}\")\n",
        "print(f\"Features: {len(data.columns) - 3} (excluding station_id, NSE_val, Performance)\")\n",
        "print(f\"\\n✅ Data merge complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM4IadZSfUzX"
      },
      "source": [
        "## 2. Comprehensive Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlnO6JBAfUzY",
        "outputId": "b1cfe8bd-9dc3-4289-8d03-360dadc2b047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "FEATURE ENGINEERING\n",
            "================================================================================\n",
            "✓ Catchment area\n",
            "✓ Snow fraction\n",
            "✓ Rainfall annual\n",
            "✓ Q/P ratio and wet/dry classification\n",
            "  Wet catchments (Q/P > 0.5): 129\n",
            "  Dry catchments (Q/P < 0.3): 142\n",
            "  Balanced (0.3 ≤ Q/P ≤ 0.5): 278\n",
            "✓ Aridity indices\n",
            "✓ BFI mean (3 methods)\n",
            "✓ Flashiness index\n",
            "✓ Altitude (km)\n",
            "✓ Geographic zones\n",
            "✓ Temperature-precipitation ratio\n",
            "✓ Seasonality index\n",
            "✓ Dam indicator\n",
            "\n",
            "✓ Feature engineering complete\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. CATCHMENT AREA\n",
        "if 'sta_area_snap' in data.columns:\n",
        "    data['catchment_area'] = data['sta_area_snap']\n",
        "    data['log_area'] = np.log10(data['catchment_area'].clip(lower=1))\n",
        "    print(\"✓ Catchment area\")\n",
        "\n",
        "# 2. SNOW FRACTION\n",
        "if 'cli_psol_frac_safran' in data.columns:\n",
        "    data['snow_fraction'] = data['cli_psol_frac_safran']\n",
        "    print(\"✓ Snow fraction\")\n",
        "\n",
        "# 3. RAINFALL (convert mm/day to mm/year if needed)\n",
        "if 'cli_prec_mean_yr' in data.columns:\n",
        "    data['rainfall_annual'] = data['cli_prec_mean_yr']\n",
        "elif 'cli_prec_mean' in data.columns:\n",
        "    # cli_prec_mean is in mm/day, convert to mm/year\n",
        "    data['rainfall_annual'] = data['cli_prec_mean'] * 365.25\n",
        "print(\"✓ Rainfall annual\")\n",
        "\n",
        "# 4. Q/P RATIO (RUNOFF COEFFICIENT)\n",
        "if 'hyd_q_mean' in data.columns and 'cli_prec_mean' in data.columns:\n",
        "    # Both in mm/day\n",
        "    data['qp_ratio'] = data['hyd_q_mean'] / (data['cli_prec_mean'] + 0.01)\n",
        "    data['qp_ratio'] = data['qp_ratio'].clip(0, 1)  # Physical bounds [0, 1]\n",
        "\n",
        "    # Wet vs Dry classification\n",
        "    data['catchment_type_wet'] = (data['qp_ratio'] > 0.5).astype(int)  # Wet\n",
        "    data['catchment_type_dry'] = (data['qp_ratio'] < 0.3).astype(int)  # Dry\n",
        "    # Balanced is reference (both zeros)\n",
        "\n",
        "    print(\"✓ Q/P ratio and wet/dry classification\")\n",
        "    print(f\"  Wet catchments (Q/P > 0.5): {data['catchment_type_wet'].sum()}\")\n",
        "    print(f\"  Dry catchments (Q/P < 0.3): {data['catchment_type_dry'].sum()}\")\n",
        "    print(f\"  Balanced (0.3 ≤ Q/P ≤ 0.5): {((data['qp_ratio'] >= 0.3) & (data['qp_ratio'] <= 0.5)).sum()}\")\n",
        "\n",
        "# 5. ARIDITY INDEX\n",
        "if 'cli_pet_ou_mean' in data.columns and 'cli_prec_mean' in data.columns:\n",
        "    data['aridity_index'] = data['cli_pet_ou_mean'] / (data['cli_prec_mean'] + 0.01)\n",
        "    data['moisture_index'] = (data['cli_prec_mean'] - data['cli_pet_ou_mean']) / (data['cli_pet_ou_mean'] + 0.01)\n",
        "    print(\"✓ Aridity indices\")\n",
        "\n",
        "# 6. BASEFLOW INDEX (AVERAGE)\n",
        "bfi_cols = [col for col in data.columns if 'bfi' in col.lower() and data[col].dtype in [np.float64, np.int64]]\n",
        "if len(bfi_cols) > 0:\n",
        "    data['bfi_mean'] = data[bfi_cols].mean(axis=1)\n",
        "    print(f\"✓ BFI mean ({len(bfi_cols)} methods)\")\n",
        "\n",
        "# 7. FLASHINESS INDEX\n",
        "if 'top_slo_mean' in data.columns and 'catchment_area' in data.columns:\n",
        "    data['flashiness_index'] = data['top_slo_mean'] / (np.sqrt(data['catchment_area']) + 1)\n",
        "    print(\"✓ Flashiness index\")\n",
        "\n",
        "# 8. ALTITUDE\n",
        "if 'top_altitude_mean' in data.columns:\n",
        "    data['altitude_km'] = data['top_altitude_mean'] / 1000\n",
        "    print(\"✓ Altitude (km)\")\n",
        "\n",
        "# 9. GEOGRAPHIC ZONES\n",
        "if all(col in data.columns for col in ['sta_x_l93', 'sta_y_l93', 'top_altitude_mean']):\n",
        "    x, y, alt = data['sta_x_l93'], data['sta_y_l93'], data['top_altitude_mean']\n",
        "\n",
        "    data['zone_Alpine'] = ((alt > 800) & (x > 800000)).astype(int)\n",
        "    data['zone_Mediterranean'] = ((y < 6200000) & (alt < 500)).astype(int)\n",
        "    data['zone_Atlantic'] = ((x < 600000) & (data['zone_Alpine'] == 0)).astype(int)\n",
        "\n",
        "    print(\"✓ Geographic zones\")\n",
        "\n",
        "# 10. TEMPERATURE-PRECIPITATION INTERACTION\n",
        "if 'cli_temp_mean' in data.columns and 'cli_prec_mean' in data.columns:\n",
        "    data['temp_prec_ratio'] = data['cli_temp_mean'] / (data['cli_prec_mean'] + 0.01)\n",
        "    print(\"✓ Temperature-precipitation ratio\")\n",
        "\n",
        "# 11. SEASONALITY INDEX\n",
        "if 'cli_prec_season_temp' in data.columns:\n",
        "    data['seasonality_index'] = data['cli_prec_season_temp']\n",
        "    print(\"✓ Seasonality index\")\n",
        "\n",
        "# 12. HUMAN INFLUENCE\n",
        "if 'dam_n' in data.columns:\n",
        "    data['has_dams'] = (data['dam_n'] > 0).astype(int)\n",
        "    print(\"✓ Dam indicator\")\n",
        "\n",
        "print(f\"\\n✓ Feature engineering complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ixFKkJmfUzZ"
      },
      "source": [
        "## 3. Comprehensive Descriptive Analyses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMZf8_IzfUza",
        "outputId": "c3610a40-40b6-471c-8eac-3c03cbbe3e9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantitative variables: 175\n"
          ]
        }
      ],
      "source": [
        "# Exclude non-predictive columns\n",
        "exclude_patterns = [\n",
        "    'station_id', 'NSE_val', 'Performance', 'NSE_train', 'RMSE',\n",
        "    'label', 'code', 'comment', 'name', 'date', 'mnemonic', 'usual',\n",
        "    'test', 'child', 'parent', 'timing', 'monitor', '_add', 'qual_'\n",
        "]\n",
        "\n",
        "quant_vars = []\n",
        "for col in data.columns:\n",
        "    if any(pattern in col.lower() for pattern in exclude_patterns):\n",
        "        continue\n",
        "    if data[col].dtype in [np.float64, np.int64]:\n",
        "        if data[col].notna().sum() > len(data) * 0.3:\n",
        "            if data[col].std() > 1e-10:\n",
        "                quant_vars.append(col)\n",
        "\n",
        "print(f\"Quantitative variables: {len(quant_vars)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h8HHd9JfUzc",
        "outputId": "109cbb53-3fd9-42be-d0b8-2fa158aafa40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "1. RAINFALL EFFECT ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Rainfall vs NSE:\n",
            "  Spearman ρ = +0.344 (p = 9.977e-17)\n",
            "  Range: [655, 2078] mm/year\n",
            "  ✓ SIGNIFICANT: Higher rainfall → BETTER performance\n",
            "\n",
            "Performance by Rainfall Tertile:\n",
            "  Low          [655, 905] mm/yr: NSE = 0.606 ± 0.132 (n=183, VG=2.7%)\n",
            "  Medium      [906, 1064] mm/yr: NSE = 0.648 ± 0.174 (n=183, VG=12.6%)\n",
            "  High       [1065, 2078] mm/yr: NSE = 0.701 ± 0.135 (n=183, VG=20.2%)\n",
            "  ANOVA: F = 18.99, p = 1.061e-08\n",
            "  ✓ SIGNIFICANT: Rainfall regime affects performance\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"1. RAINFALL EFFECT ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if 'rainfall_annual' in data.columns:\n",
        "    valid = data[['NSE_val', 'rainfall_annual']].dropna()\n",
        "    if len(valid) > 30:\n",
        "        r, p = stats.spearmanr(valid['NSE_val'], valid['rainfall_annual'])\n",
        "        print(f\"\\nRainfall vs NSE:\")\n",
        "        print(f\"  Spearman ρ = {r:+.3f} (p = {p:.3e})\")\n",
        "        print(f\"  Range: [{valid['rainfall_annual'].min():.0f}, {valid['rainfall_annual'].max():.0f}] mm/year\")\n",
        "\n",
        "        if p < 0.05:\n",
        "            if r > 0:\n",
        "                print(f\"  ✓ SIGNIFICANT: Higher rainfall → BETTER performance\")\n",
        "            else:\n",
        "                print(f\"  ✓ SIGNIFICANT: Higher rainfall → WORSE performance\")\n",
        "        else:\n",
        "            print(f\"  ✗ Not significant\")\n",
        "\n",
        "        # Tertile analysis\n",
        "        data['rainfall_tertile'] = pd.qcut(data['rainfall_annual'], q=3, labels=['Low', 'Medium', 'High'], duplicates='drop')\n",
        "        print(f\"\\nPerformance by Rainfall Tertile:\")\n",
        "        for tertile in ['Low', 'Medium', 'High']:\n",
        "            subset = data[data['rainfall_tertile'] == tertile]\n",
        "            if len(subset) > 0:\n",
        "                mean_nse = subset['NSE_val'].mean()\n",
        "                std_nse = subset['NSE_val'].std()\n",
        "                rain_range = f\"[{subset['rainfall_annual'].min():.0f}, {subset['rainfall_annual'].max():.0f}]\"\n",
        "\n",
        "                # Calculate VG% (NSE > 0.80)\n",
        "                vg_count = (subset['NSE_val'] > 0.80).sum()\n",
        "                vg_pct = 100 * vg_count / len(subset)\n",
        "\n",
        "                print(f\"  {tertile:6s} {rain_range:>16s} mm/yr: NSE = {mean_nse:.3f} ± {std_nse:.3f} (n={len(subset):3d}, VG={vg_pct:.1f}%)\")\n",
        "\n",
        "        # ANOVA test\n",
        "        groups = [data[data['rainfall_tertile'] == tertile]['NSE_val'].dropna()\n",
        "                  for tertile in ['Low', 'Medium', 'High']\n",
        "                  if len(data[data['rainfall_tertile'] == tertile]) > 0]\n",
        "\n",
        "        if len(groups) >= 2:\n",
        "            f_stat, p_anova = stats.f_oneway(*groups)\n",
        "            print(f\"  ANOVA: F = {f_stat:.2f}, p = {p_anova:.3e}\")\n",
        "            if p_anova < 0.05:\n",
        "                print(f\"  ✓ SIGNIFICANT: Rainfall regime affects performance\")\n",
        "            else:\n",
        "                print(f\"  ✗ Not significant\")\n",
        "else:\n",
        "    print(\"⚠️ Rainfall data not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz0UdCN8fUzd",
        "outputId": "b97379d5-9722-44e2-b793-aa42ab4318ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "2. WET vs DRY CATCHMENT ANALYSIS (Q/P RATIO)\n",
            "================================================================================\n",
            "\n",
            "Q/P Ratio vs NSE:\n",
            "  Spearman ρ = +0.355 (p = 8.527e-18)\n",
            "  Range: [0.054, 1.000]\n",
            "  ✓ SIGNIFICANT: Wetter catchments (high Q/P) → BETTER performance\n",
            "\n",
            "Performance by Catchment Type:\n",
            "  Wet (Q/P > 0.5)     : NSE = 0.680 ± 0.172 (n=129, VG=24.0%)\n",
            "  Balanced (0.3-0.5)  : NSE = 0.681 ± 0.126 (n=278, VG=12.2%)\n",
            "  Dry (Q/P < 0.3)     : NSE = 0.567 ± 0.153 (n=142, VG=0.0%)\n",
            "\n",
            "  ANOVA: F = 32.57, p = 4.356e-14\n",
            "  ✓ SIGNIFICANT: Catchment wetness affects performance\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"2. WET vs DRY CATCHMENT ANALYSIS (Q/P RATIO)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if 'qp_ratio' in data.columns:\n",
        "    # Correlation\n",
        "    valid = data[['NSE_val', 'qp_ratio']].dropna()\n",
        "    if len(valid) > 30:\n",
        "        r, p = stats.spearmanr(valid['NSE_val'], valid['qp_ratio'])\n",
        "        print(f\"\\nQ/P Ratio vs NSE:\")\n",
        "        print(f\"  Spearman ρ = {r:+.3f} (p = {p:.3e})\")\n",
        "        print(f\"  Range: [{valid['qp_ratio'].min():.3f}, {valid['qp_ratio'].max():.3f}]\")\n",
        "\n",
        "        if p < 0.05:\n",
        "            if r > 0:\n",
        "                print(f\"  ✓ SIGNIFICANT: Wetter catchments (high Q/P) → BETTER performance\")\n",
        "            else:\n",
        "                print(f\"  ✓ SIGNIFICANT: Drier catchments (low Q/P) → BETTER performance (unexpected!)\")\n",
        "        else:\n",
        "            print(f\"  ✗ Not significant\")\n",
        "\n",
        "    # Compare wet vs dry vs balanced\n",
        "    print(f\"\\nPerformance by Catchment Type:\")\n",
        "\n",
        "    wet = data[data['catchment_type_wet'] == 1]\n",
        "    dry = data[data['catchment_type_dry'] == 1]\n",
        "    balanced = data[(data['catchment_type_wet'] == 0) & (data['catchment_type_dry'] == 0)]\n",
        "\n",
        "    for ctype, subset in [('Wet (Q/P > 0.5)', wet), ('Balanced (0.3-0.5)', balanced), ('Dry (Q/P < 0.3)', dry)]:\n",
        "        if len(subset) > 0:\n",
        "            mean_nse = subset['NSE_val'].mean()\n",
        "            std_nse = subset['NSE_val'].std()\n",
        "            pct_vg = 100 * (subset['Performance'] == 'Very Good').sum() / len(subset)\n",
        "            print(f\"  {ctype:20s}: NSE = {mean_nse:.3f} ± {std_nse:.3f} (n={len(subset):3d}, VG={pct_vg:.1f}%)\")\n",
        "\n",
        "    # ANOVA\n",
        "    groups = [wet['NSE_val'].dropna(), balanced['NSE_val'].dropna(), dry['NSE_val'].dropna()]\n",
        "    if all(len(g) > 0 for g in groups):\n",
        "        f_stat, p_val = stats.f_oneway(*groups)\n",
        "        print(f\"\\n  ANOVA: F = {f_stat:.2f}, p = {p_val:.3e}\")\n",
        "        if p_val < 0.05:\n",
        "            print(f\"  ✓ SIGNIFICANT: Catchment wetness affects performance\")\n",
        "else:\n",
        "    print(\"⚠️ Q/P ratio not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW71KFhZfUzd",
        "outputId": "37e816c8-2766-4148-c01b-1512211a27ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "3. SNOW EFFECT ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Snow Fraction vs NSE:\n",
            "  Spearman ρ = -0.192 (p = 5.674e-06)\n",
            "  Range: [0.004, 0.524]\n",
            "  ✓ SIGNIFICANT: Higher snow → WORSE performance\n",
            "\n",
            "Performance by Snow Regime:\n",
            "  Negligible (0-0.02) : NSE = 0.699 ± 0.140 (n=157, VG=23.6%)\n",
            "  Low (0.02-0.1)      : NSE = 0.639 ± 0.149 (n=278, VG=6.8%)\n",
            "  Moderate (0.1-0.5)  : NSE = 0.621 ± 0.157 (n=113, VG=8.0%)\n",
            "  High (> 0.5)        : NSE = 0.008 ± nan (n=1, VG=0.0%)\n",
            "  ANOVA: F = 13.98, p = 8.623e-09\n",
            "  ✓ SIGNIFICANT: Snow regime affects performance\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"3. SNOW EFFECT ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if 'snow_fraction' in data.columns:\n",
        "    valid = data[['NSE_val', 'snow_fraction']].dropna()\n",
        "    if len(valid) > 30:\n",
        "        r, p = stats.spearmanr(valid['NSE_val'], valid['snow_fraction'])\n",
        "        print(f\"\\nSnow Fraction vs NSE:\")\n",
        "        print(f\"  Spearman ρ = {r:+.3f} (p = {p:.3e})\")\n",
        "        print(f\"  Range: [{valid['snow_fraction'].min():.3f}, {valid['snow_fraction'].max():.3f}]\")\n",
        "\n",
        "        if p < 0.05:\n",
        "            if r < 0:\n",
        "                print(f\"  ✓ SIGNIFICANT: Higher snow → WORSE performance\")\n",
        "            else:\n",
        "                print(f\"  ✓ SIGNIFICANT: Higher snow → BETTER performance\")\n",
        "        else:\n",
        "            print(f\"  ✗ Not significant\")\n",
        "\n",
        "    # Categorize by snow fraction (hydrological thresholds)\n",
        "    data['snow_category'] = pd.cut(\n",
        "        data['snow_fraction'],\n",
        "        bins=[0, 0.02, 0.1, 0.5, 1.0],\n",
        "        labels=['Negligible', 'Low', 'Moderate', 'High'],\n",
        "        include_lowest=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\nPerformance by Snow Regime:\")\n",
        "\n",
        "    for category in ['Negligible', 'Low', 'Moderate', 'High']:\n",
        "        subset = data[data['snow_category'] == category]\n",
        "        if len(subset) > 0:\n",
        "            mean_nse = subset['NSE_val'].mean()\n",
        "            std_nse = subset['NSE_val'].std()\n",
        "            snow_range = f\"[{subset['snow_fraction'].min():.3f}, {subset['snow_fraction'].max():.3f}]\"\n",
        "\n",
        "            # Calculate VG% (NSE > 0.80)\n",
        "            vg_count = (subset['NSE_val'] > 0.80).sum()\n",
        "            vg_pct = 100 * vg_count / len(subset)\n",
        "\n",
        "            # Format category with threshold\n",
        "            if category == 'Negligible':\n",
        "                cat_label = f\"{category} (0-0.02)\"\n",
        "            elif category == 'Low':\n",
        "                cat_label = f\"{category} (0.02-0.1)\"\n",
        "            elif category == 'Moderate':\n",
        "                cat_label = f\"{category} (0.1-0.5)\"\n",
        "            else:\n",
        "                cat_label = f\"{category} (> 0.5)\"\n",
        "\n",
        "            print(f\"  {cat_label:20s}: NSE = {mean_nse:.3f} ± {std_nse:.3f} (n={len(subset)}, VG={vg_pct:.1f}%)\")\n",
        "\n",
        "    # ANOVA test\n",
        "    groups = [data[data['snow_category'] == cat]['NSE_val'].dropna()\n",
        "              for cat in ['Negligible', 'Low', 'Moderate', 'High']\n",
        "              if len(data[data['snow_category'] == cat]) > 0]\n",
        "\n",
        "    if len(groups) >= 2:\n",
        "        f_stat, p_anova = stats.f_oneway(*groups)\n",
        "        print(f\"  ANOVA: F = {f_stat:.2f}, p = {p_anova:.3e}\")\n",
        "        if p_anova < 0.05:\n",
        "            print(f\"  ✓ SIGNIFICANT: Snow regime affects performance\")\n",
        "        else:\n",
        "            print(f\"  ✗ Not significant\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2qpfljXfUze",
        "outputId": "d19222a8-5a12-41c5-95c9-97ea20814d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "4. GEOGRAPHIC ZONE ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Performance by Zone:\n",
            "  Alpine         : NSE = 0.553 ± 0.218 (n= 38, VG=2.6%)\n",
            "  Atlantic       : NSE = 0.689 ± 0.136 (n=209, VG=21.1%)\n",
            "  Continental    : NSE = 0.638 ± 0.148 (n=302, VG=6.6%)\n",
            "\n",
            "  ANOVA: F = 16.04, p = 1.701e-07\n",
            "  ✓ SIGNIFICANT: Geographic zone affects performance\n",
            "\n",
            "✓ Saved: XGBoost_Expert_01_Geographic_Zones.csv\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"4. GEOGRAPHIC ZONE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if 'zone_Alpine' in data.columns:\n",
        "    zones = [\n",
        "        ('Alpine', data['zone_Alpine'] == 1),\n",
        "        ('Mediterranean', data['zone_Mediterranean'] == 1),\n",
        "        ('Atlantic', data['zone_Atlantic'] == 1),\n",
        "        ('Continental', ~((data['zone_Alpine'] == 1) | (data['zone_Mediterranean'] == 1) | (data['zone_Atlantic'] == 1)))\n",
        "    ]\n",
        "\n",
        "    zone_stats = []\n",
        "    print(f\"\\nPerformance by Zone:\")\n",
        "    for zone_name, mask in zones:\n",
        "        subset = data[mask]\n",
        "        if len(subset) > 0:\n",
        "            mean_nse = subset['NSE_val'].mean()\n",
        "            std_nse = subset['NSE_val'].std()\n",
        "            pct_vg = 100 * (subset['Performance'] == 'Very Good').sum() / len(subset)\n",
        "\n",
        "            print(f\"  {zone_name:15s}: NSE = {mean_nse:.3f} ± {std_nse:.3f} (n={len(subset):3d}, VG={pct_vg:.1f}%)\")\n",
        "\n",
        "            zone_stats.append({'Zone': zone_name, 'n': len(subset), 'Mean_NSE': mean_nse,\n",
        "                              'Std_NSE': std_nse, 'Pct_VeryGood': pct_vg})\n",
        "\n",
        "    # ANOVA\n",
        "    zone_groups = [data[mask]['NSE_val'].dropna() for _, mask in zones if mask.sum() > 0]\n",
        "    if len(zone_groups) > 2:\n",
        "        f_stat, p_val = stats.f_oneway(*zone_groups)\n",
        "        print(f\"\\n  ANOVA: F = {f_stat:.2f}, p = {p_val:.3e}\")\n",
        "        if p_val < 0.05:\n",
        "            print(f\"  ✓ SIGNIFICANT: Geographic zone affects performance\")\n",
        "\n",
        "    pd.DataFrame(zone_stats).to_csv('XGBoost_Expert_01_Geographic_Zones.csv', index=False)\n",
        "    print(\"\\n✓ Saved: XGBoost_Expert_01_Geographic_Zones.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY3B423TfUzf",
        "outputId": "c9eabaee-16b1-4eb1-f850-4c51c815542e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "5. CATCHMENT SIZE ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Catchment Area vs NSE:\n",
            "  Spearman ρ = +0.098 (p = 2.140e-02)\n",
            "  Range: [7.0, 110188.2] km²\n",
            "  ✓ SIGNIFICANT: Larger catchments → BETTER performance\n",
            "\n",
            "Performance by Catchment Size:\n",
            "  Small (< 100 km²)        : NSE = 0.636 ± 0.176 (n=143, VG=19.6%)\n",
            "  Medium (100-1000 km²)    : NSE = 0.647 ± 0.147 (n=338, VG=8.6%)\n",
            "  Large (> 1000 km²)       : NSE = 0.706 ± 0.117 (n=68, VG=11.8%)\n",
            "  ANOVA: F = 5.34, p = 5.055e-03\n",
            "  ✓ SIGNIFICANT: Catchment size affects performance\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"5. CATCHMENT SIZE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if 'catchment_area' in data.columns:\n",
        "    valid = data[['NSE_val', 'catchment_area']].dropna()\n",
        "    if len(valid) > 30:\n",
        "        r, p = stats.spearmanr(valid['NSE_val'], valid['catchment_area'])\n",
        "        print(f\"\\nCatchment Area vs NSE:\")\n",
        "        print(f\"  Spearman ρ = {r:+.3f} (p = {p:.3e})\")\n",
        "        print(f\"  Range: [{valid['catchment_area'].min():.1f}, {valid['catchment_area'].max():.1f}] km²\")\n",
        "\n",
        "        if p < 0.05:\n",
        "            if r > 0:\n",
        "                print(f\"  ✓ SIGNIFICANT: Larger catchments → BETTER performance\")\n",
        "            else:\n",
        "                print(f\"  ✓ SIGNIFICANT: Smaller catchments → BETTER performance\")\n",
        "        else:\n",
        "            print(f\"  ✗ Not significant\")\n",
        "\n",
        "        # Categorize by size (hydrological thresholds)\n",
        "        data['size_category'] = pd.cut(\n",
        "            data['catchment_area'],\n",
        "            bins=[0, 100, 1000, float('inf')],\n",
        "            labels=['Small', 'Medium', 'Large']\n",
        "        )\n",
        "\n",
        "        print(f\"\\nPerformance by Catchment Size:\")\n",
        "\n",
        "        for category in ['Small', 'Medium', 'Large']:\n",
        "            subset = data[data['size_category'] == category]\n",
        "            if len(subset) > 0:\n",
        "                mean_nse = subset['NSE_val'].mean()\n",
        "                std_nse = subset['NSE_val'].std()\n",
        "                area_range = f\"[{subset['catchment_area'].min():.1f}, {subset['catchment_area'].max():.1f}]\"\n",
        "\n",
        "                # Calculate VG% (NSE > 0.80)\n",
        "                vg_count = (subset['NSE_val'] > 0.80).sum()\n",
        "                vg_pct = 100 * vg_count / len(subset)\n",
        "\n",
        "                # Format category with threshold\n",
        "                if category == 'Small':\n",
        "                    cat_label = f\"{category} (< 100 km²)\"\n",
        "                elif category == 'Medium':\n",
        "                    cat_label = f\"{category} (100-1000 km²)\"\n",
        "                else:\n",
        "                    cat_label = f\"{category} (> 1000 km²)\"\n",
        "\n",
        "                print(f\"  {cat_label:25s}: NSE = {mean_nse:.3f} ± {std_nse:.3f} (n={len(subset)}, VG={vg_pct:.1f}%)\")\n",
        "\n",
        "        # ANOVA test\n",
        "        groups = [data[data['size_category'] == cat]['NSE_val'].dropna()\n",
        "                  for cat in ['Small', 'Medium', 'Large']\n",
        "                  if len(data[data['size_category'] == cat]) > 0]\n",
        "\n",
        "        if len(groups) >= 2:\n",
        "            f_stat, p_anova = stats.f_oneway(*groups)\n",
        "            print(f\"  ANOVA: F = {f_stat:.2f}, p = {p_anova:.3e}\")\n",
        "            if p_anova < 0.05:\n",
        "                print(f\"  ✓ SIGNIFICANT: Catchment size affects performance\")\n",
        "            else:\n",
        "                print(f\"  ✗ Not significant\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}